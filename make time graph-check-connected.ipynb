{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "658483c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import datetime\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b817d5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path=\"save_graphs\\\\time_graph\\\\\"\n",
    "\n",
    "# {\"original_tweet's user ID\": [tweets that were replies to the original tweets by this user]}\n",
    "in_reply_to_user_dict = {}\n",
    "\n",
    "# {\"original Tweet’s ID\": [tweets that were replies to the original tweet]}\n",
    "in_reply_to_tweet_dict = {}\n",
    "\n",
    "user_dict_file = save_path+\"in_reply_to_user_dict_delete.txt\"\n",
    "tweet_dict_file = save_path+\"in_reply_to_tweet_dict_delete.txt\"\n",
    "edges_record = save_path+\"edge_record.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddba5614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "577710"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_set = set()\n",
    "with open(save_path+\"check_connected_components.txt\",\"r\") as check_connected:\n",
    "    for element in check_connected:\n",
    "        tweet_set.add(element[:-1])\n",
    "len(tweet_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "576e55c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes present\n"
     ]
    }
   ],
   "source": [
    "if '1364413096819896320' in tweet_set:\n",
    "    print(\"yes present\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95ab6ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_records(file_name):\n",
    "    \"\"\"\n",
    "    returns the total number of tweets in the file\n",
    "    we are having a single tweet per row - each having a unique tweet id\n",
    "    \"\"\"\n",
    "    with open(file_name,\"r\") as node_list:\n",
    "        records=0\n",
    "        for rows in node_list:\n",
    "            records+=1\n",
    "        return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da4e861f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printl(text):\n",
    "    with open(save_path+\"time-graph-logs.txt\",\"a\") as log_file:\n",
    "        log_file.write(f\"{datetime.datetime.now().strftime('%c')}   {text}\")\n",
    "        log_file.write(\"\\n\")\n",
    "        \n",
    "def write_dicts():\n",
    "    with open(user_dict_file,\"w\") as user_dict:\n",
    "        for user_id in in_reply_to_user_dict.keys():\n",
    "            user_dict.write(user_id+\"\\t\"+str(in_reply_to_user_dict[user_id]))\n",
    "            user_dict.write(\"\\n\")\n",
    "            \n",
    "    with open(tweet_dict_file,\"w\") as tweet_dict:\n",
    "        for tweet_id in in_reply_to_tweet_dict.keys():\n",
    "            tweet_dict.write(tweet_id+\"\\t\"+str(in_reply_to_tweet_dict[tweet_id]))\n",
    "            tweet_dict.write(\"\\n\")\n",
    "\n",
    "def get_int_timestamp(dt_string):\n",
    "    dt_items = dt_string.split(\" \")\n",
    "    date_string = dt_items[0]+\"-\"+dt_items[1]+\"-\"+dt_items[2]+\"-\"+dt_items[5]+\"-\"+dt_items[3]\n",
    "    date_object = datetime.datetime.strptime(date_string, \"%a-%b-%d-%Y-%H:%M:%S\")\n",
    "    return int(date_object.timestamp())\n",
    "\n",
    "def look_at_all_files(func):\n",
    "    def inner(*args, **kwargs):\n",
    "        printl(f\"looking at .tsv files in directory {args[0]} for [{args[1]}]\")\n",
    "        files = glob.glob(args[0]+'[hydrated-tweets]*.tsv')\n",
    "        for file in files:\n",
    "            kwargs[\"file_name\"]=file\n",
    "            func(*args, **kwargs)\n",
    "    return inner\n",
    "\n",
    "def add_edge_record(source,target,first=\"no\", time=None, tweet_relation=None):\n",
    "    with open(edges_record,\"a\") as edge:\n",
    "        if first==\"yes\":\n",
    "            edge.write(\"source\"+\"\\t\"+\"destination\"+\"\\t\"+\"time\"+\"\\t\"+\"tweet_relation\")\n",
    "            edge.write(\"\\n\")\n",
    "        elif time is not None:\n",
    "            edge.write(source+\"\\t\"+target+\"\\t\"+time+\"\\t\"+\"none\")\n",
    "            edge.write(\"\\n\")\n",
    "        elif tweet_relation is not None:\n",
    "            edge.write(source+\"\\t\"+target+\"\\t\"+\"none\"+\"\\t\"+tweet_relation)\n",
    "            edge.write(\"\\n\")\n",
    "            \n",
    "add_edge_record(\"a\",\"b\",first=\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b7ccd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "@look_at_all_files\n",
    "def add_tweet_nodes(directory, purpose, **kwargs):\n",
    "    \"\"\"\n",
    "    it will go over all the tsv tweet files in data folder and add\n",
    "    each tweet as a node in graph\n",
    "    \n",
    "    calling format\n",
    "    add_tweet_nodes(directory, purpose, graph=\"input_graph\")\n",
    "    \"\"\"\n",
    "    \n",
    "    file_name = kwargs[\"file_name\"]\n",
    "    graph = kwargs[\"graph\"]\n",
    "    printl(f\"total records in {file_name} are {total_records(file_name)}\")\n",
    "    \n",
    "    with open(file_name,\"r\") as node_list:\n",
    "        for rows in node_list:\n",
    "            \n",
    "            cols = rows.split(\"\\t\")\n",
    "            if cols[0] == \"tweet_id\":\n",
    "                continue\n",
    "            if cols[0] in tweet_set:\n",
    "                graph.add_node(cols[0])\n",
    "                if cols[4] != \"\":\n",
    "                    if in_reply_to_user_dict.get(cols[4], \"not added\") == \"not added\":\n",
    "                        in_reply_to_user_dict[cols[4]]=[(cols[0],cols[5])]\n",
    "                    else:\n",
    "                        in_reply_to_user_dict[cols[4]].append((cols[0],cols[5]))\n",
    "\n",
    "                if cols[3] != \"\":\n",
    "                    #if the represented Tweet is a reply\n",
    "                    #this field will contain the string representation of the original Tweet’s ID.\n",
    "                    #so that means edge can go from current tweet ID (later) to in_reply_to_status_id_str (earlier) \n",
    "                    #the tweet node part of edge will automatically be added to graph if not present\n",
    "                    graph.add_edge(cols[3],cols[0])\n",
    "                    add_edge_record(cols[3],cols[0],tweet_relation=\"yes\")\n",
    "\n",
    "                    if in_reply_to_tweet_dict.get(cols[3], \"not added\") == \"not added\":\n",
    "                        in_reply_to_tweet_dict[cols[3]]=[cols[0]]\n",
    "                    else:\n",
    "                        in_reply_to_tweet_dict[cols[3]].append(cols[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "709475e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@look_at_all_files\n",
    "def make_edges_based_user(directory, purpose, **kwargs):\n",
    "    \"\"\"\n",
    "    it will go over all the tsv tweet files in data folder and add\n",
    "    edges based on common user between tweets\n",
    "    \n",
    "    calling format\n",
    "    make_edges_based_user(directory, purpose, graph=\"input_graph\")\n",
    "    \"\"\"\n",
    "    #If the represented Tweet is a reply\n",
    "    #this field will contain the string representation of the original Tweet’s author ID.\n",
    "    #in_reply_to_user_id_str\n",
    "    \n",
    "    # add edges in the graph based on created at timestamp\n",
    "    \n",
    "    file_name = kwargs[\"file_name\"]\n",
    "    graph = kwargs[\"graph\"]\n",
    "    \n",
    "    with open(file_name,\"r\") as node_list:\n",
    "        count=0\n",
    "        for rows in node_list:\n",
    "            count+=1\n",
    "            if count%20000 == 0:\n",
    "                printl(f\"making edges - have read {count} entries in {file_name}\")\n",
    "            cols = rows.split(\"\\t\")\n",
    "            if cols[0] == \"tweet_id\":\n",
    "                continue\n",
    "            if in_reply_to_user_dict.get(cols[2], \"not found\") != \"not found\":\n",
    "                tweet_1 = cols[0]#tweet ID of tweet by user\n",
    "                tweet_1_created_at = cols[5]#created at time of tweet by user\n",
    "                for tweet_time in in_reply_to_user_dict.get(cols[2]): #we are trying to find user_id of current tweet in the dict\n",
    "                    if get_int_timestamp(tweet_1_created_at)>get_int_timestamp(tweet_time[1]):\n",
    "                        #tweet_1 is at later time\n",
    "                        #edge from earlier to later\n",
    "                        graph.add_edge(tweet_time[0], tweet_1)\n",
    "                        add_edge_record(tweet_time[0], tweet_1, time=str(tweet_time[1])+\"$\"+str(tweet_1_created_at))\n",
    "                    elif get_int_timestamp(tweet_1_created_at) == get_int_timestamp(tweet_time[1]):\n",
    "                        #do not consider these nodes\n",
    "                        #do not add edge\n",
    "                        with open(save_path+\"same_created_time_tweets_delete.txt\",\"a\") as same_time_tweets:\n",
    "                            same_time_tweets.write(f\"tweet 1 - {tweet_1}    tweet 2 - {tweet_time[0]}    created_at_time1 - {tweet_1_created_at}    created_at_time2 - {tweet_time[1]}\")\n",
    "                            same_time_tweets.write(\"\\n\")\n",
    "                    else:\n",
    "                        #tweet 2 at later time\n",
    "                        graph.add_edge(tweet_1, tweet_time[0])\n",
    "                        add_edge_record(tweet_1, tweet_time[0], time=str(tweet_1_created_at)+\"$\"+str(tweet_time[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c2ddd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44ad0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to graph all the tweet nodes\n",
    "\n",
    "# iterate through each day's tsv file and add node for each tweet id str\n",
    "# add edges from tweet nodes to original tweets which were replied to\n",
    "add_tweet_nodes(\"data1\\\\\", \"adding tweet nodes\", graph=G)\n",
    "printl(\"added the tweet nodes\")\n",
    "# write the dicts to disk\n",
    "write_dicts()\n",
    "\n",
    "printl(f\"the Total number of entries from all tsv files in in_reply_to_user_dict keys is {len(in_reply_to_user_dict.keys())}\")\n",
    "printl(f\"the Total number of entries from all tsv files in in_reply_to_tweet_dict keys is {len(in_reply_to_tweet_dict.keys())}\")\n",
    "\n",
    "\n",
    "# make edges based on common users\n",
    "# directed based on created at time\n",
    "make_edges_based_user(\"data\\\\\", \"make edges based on common users\", graph=G)\n",
    "printl(\"added edges among tweet nodes based on user relation\")\n",
    "\n",
    "printl(f\"the tweets which were being connected based on common user relation but were dropped due to same creation time entry - {total_records(save_path+'same_created_time_tweets.txt')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "625029ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gpickle(G, save_path+\"time_graph_delete.gpickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318d635d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
