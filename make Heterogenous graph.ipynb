{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3db16edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import datetime\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9765147b",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path=\"save_graphs\\\\heterogenous_graph\\\\\"\n",
    "\n",
    "# {\"original_tweet's user ID\": [tweets that were replies to the original tweets by this user]}\n",
    "in_reply_to_user_dict = {}\n",
    "\n",
    "# {\"original Tweet’s ID\": [tweets that were replies to the original tweet]}\n",
    "in_reply_to_tweet_dict = {}\n",
    "\n",
    "user_dict_file = save_path+\"in_reply_to_user_dict.txt\"\n",
    "tweet_dict_file = save_path+\"in_reply_to_tweet_dict.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fa8fde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_records(file_name):\n",
    "    \"\"\"\n",
    "    returns the total number of tweets in the file\n",
    "    we are having a single tweet per row - each having a unique tweet id\n",
    "    \"\"\"\n",
    "    with open(file_name,\"r\") as node_list:\n",
    "        records=0\n",
    "        for rows in node_list:\n",
    "            records+=1\n",
    "        return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "243754ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printl(text):\n",
    "    with open(save_path+\"heterogenous-graph-logs.txt\",\"a\") as log_file:\n",
    "        log_file.write(f\"{datetime.datetime.now().strftime('%c')}   {text}\")\n",
    "        log_file.write(\"\\n\")\n",
    "        \n",
    "def write_dicts():\n",
    "    with open(user_dict_file,\"w\") as user_dict:\n",
    "        for user_id in in_reply_to_user_dict.keys():\n",
    "            user_dict.write(user_id+\"\\t\"+str(in_reply_to_user_dict[user_id]))\n",
    "            user_dict.write(\"\\n\")\n",
    "            \n",
    "    with open(tweet_dict_file,\"w\") as tweet_dict:\n",
    "        for tweet_id in in_reply_to_tweet_dict.keys():\n",
    "            tweet_dict.write(tweet_id+\"\\t\"+str(in_reply_to_tweet_dict[tweet_id]))\n",
    "            tweet_dict.write(\"\\n\")\n",
    "\n",
    "def look_at_all_files(func):\n",
    "    def inner(*args, **kwargs):\n",
    "        printl(f\"looking at .tsv files in directory {args[0]} for [{args[1]}]\")\n",
    "        files = glob.glob(args[0]+'[hydrated-tweets]*.tsv')\n",
    "        for file in files:\n",
    "            kwargs[\"file_name\"]=file\n",
    "            func(*args, **kwargs)\n",
    "    return inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67fa0112",
   "metadata": {},
   "outputs": [],
   "source": [
    "#edge in graph will be\n",
    "# tweet to original tweet (based on reply to original tweet relation)\n",
    "# tweet to user who posted the tweet (based on user who made the tweet relation)\n",
    "# tweet to original user (based on tweet made as a reply to original tweet by original user)\n",
    "# tweet to another tweet (based on common user relation - tweet posted in\n",
    "#     reply to original user - related to other tweets of the user)\n",
    "# user to original user (based on reply to original user relation)\n",
    "\n",
    "\n",
    "@look_at_all_files\n",
    "def add_all_nodes(directory, purpose, **kwargs):\n",
    "    \"\"\"\n",
    "    it will go over all the tsv tweet files in data folder and \n",
    "    i) add each tweet as a node in graph\n",
    "    ii) add each user as a node in graph\n",
    "    \n",
    "    calling format\n",
    "    add_tweet_nodes(directory, purpose, graph=\"input_graph\")\n",
    "    \"\"\"\n",
    "    \n",
    "    file_name = kwargs[\"file_name\"]\n",
    "    graph = kwargs[\"graph\"]\n",
    "    printl(f\"total records in {file_name} are {total_records(file_name)}\")\n",
    "    \n",
    "    with open(file_name,\"r\") as node_list:\n",
    "        for rows in node_list:\n",
    "            \n",
    "            cols = rows.split(\"\\t\")\n",
    "            if cols[0] == \"tweet_id\":\n",
    "                continue\n",
    "            graph.add_node(cols[0]) # add tweet node\n",
    "            graph.add_node('$'+cols[2]) # add user node\n",
    "            \n",
    "            # tweet to user who posted the tweet (based on user who made the tweet relation)\n",
    "            graph.add_edge(cols[0], '$'+cols[2])\n",
    "            \n",
    "            if cols[4] != \"\":\n",
    "                # tweet to original user (based on tweet made as a reply to original tweet by original user)\n",
    "                graph.add_edge(cols[0], '$'+cols[4])\n",
    "                \n",
    "                # user to original user (based on reply to original user relation)\n",
    "                graph.add_edge(cols[2], '$'+cols[4])\n",
    "                \n",
    "                if in_reply_to_user_dict.get('$'+cols[4], \"not added\") == \"not added\":\n",
    "                    in_reply_to_user_dict['$'+cols[4]]=[(cols[0],cols[5])]\n",
    "                else:\n",
    "                    in_reply_to_user_dict['$'+cols[4]].append((cols[0],cols[5]))\n",
    "                    \n",
    "            if cols[3] != \"\":\n",
    "                #if the represented Tweet is a reply\n",
    "                #this field will contain the string representation of the original Tweet’s ID.\n",
    "                #undirected graph\n",
    "                #so that means edge between current tweet ID (later) and in_reply_to_status_id_str (earlier) \n",
    "                #the tweet node part of edge will automatically be added to graph if not present\n",
    "                \n",
    "                # tweet to original tweet (based on reply to original tweet relation)\n",
    "                graph.add_edge(cols[3],cols[0])\n",
    "                \n",
    "                if in_reply_to_tweet_dict.get(cols[3], \"not added\") == \"not added\":\n",
    "                    in_reply_to_tweet_dict[cols[3]]=[cols[0]]\n",
    "                else:\n",
    "                    in_reply_to_tweet_dict[cols[3]].append(cols[0])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d98b9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "@look_at_all_files\n",
    "def make_edges_based_user(directory, purpose, **kwargs):\n",
    "    \"\"\"\n",
    "    it will go over all the tsv tweet files in data folder and add\n",
    "    edges based on common user between tweets\n",
    "    \n",
    "    calling format\n",
    "    make_edges_based_user(directory, purpose, graph=\"input_graph\")\n",
    "    \"\"\"\n",
    "    #If the represented Tweet is a reply\n",
    "    #this field will contain the string representation of the original Tweet’s author ID.\n",
    "    #in_reply_to_user_id_str\n",
    "    \n",
    "    file_name = kwargs[\"file_name\"]\n",
    "    graph = kwargs[\"graph\"]\n",
    "    \n",
    "    with open(file_name,\"r\") as node_list:\n",
    "        count=0\n",
    "        for rows in node_list:\n",
    "            count+=1\n",
    "            if count%20000 == 0:\n",
    "                printl(f\"making edges - have read {count} entries in {file_name}\")\n",
    "            cols = rows.split(\"\\t\")\n",
    "            if cols[0] == \"tweet_id\":\n",
    "                continue\n",
    "            if in_reply_to_user_dict.get('$'+cols[2], \"not found\") != \"not found\":\n",
    "                tweet_1 = cols[0]#tweet ID of tweet by user\n",
    "                for tweet_time in in_reply_to_user_dict.get('$'+cols[2]):\n",
    "                    graph.add_edge(tweet_time[0], tweet_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d52d9e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7efd508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to graph all the tweet nodes\n",
    "\n",
    "# iterate through each day's tsv file and add node for each tweet id str\n",
    "add_all_nodes(\"data\\\\\", \"adding all nodes\", graph=G)\n",
    "printl(\"added the tweet nodes\")\n",
    "# write the dicts to disk\n",
    "write_dicts()\n",
    "\n",
    "printl(f\"the Total number of entries from all tsv files in in_reply_to_user_dict keys is {len(in_reply_to_user_dict.keys())}\")\n",
    "printl(f\"the Total number of entries from all tsv files in in_reply_to_tweet_dict keys is {len(in_reply_to_tweet_dict.keys())}\")\n",
    "\n",
    "# make edges based on common users\n",
    "# directed based on created at time\n",
    "make_edges_based_user(\"data\\\\\", \"make edges based on common users\", graph=G)\n",
    "printl(\"added edges among tweet nodes based on user relation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90500aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gpickle(G, save_path+\"heterogenous_graph.gpickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03b1e89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
